{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSYC 193: Perception and Computation \n",
    "## Lab 4: kNN and linear classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will continue working with an image dataset used in a recent computer vision paper by [Sangkloy et al.](https://dl.acm.org/doi/abs/10.1145/2897824.2925954). \n",
    "\n",
    "**Learning objectives**\n",
    "* k-nearest-neighbor classification\n",
    "* support vector machine classification\n",
    "* logistic regression classification\n",
    "\n",
    "**Submission instructions**\n",
    "1. Please rename the notebook by replacing `YOURUSERNAME` in the filename with your actual UCSD AD username. \n",
    "2. Before submitting your assignment, sure that your notebook can run from \"top to bottom,\" executing the code in every code cell without returning fatal errors. An easy way to verify this is to click \"Kernel\" above in the tool bar, and try selecting \"Restart & Run All.\"\n",
    "3. Once you have verified that your notebook can run \"top to bottom\" without issues, click \"File\" in the toolbar above, then \"Download as,\" then \"PDF via LaTeX\" to download a PDF version of your notebook. \n",
    "4. Upload this PDF version of your notebook to Canvas before 5pm the next class period. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load generally useful python modules\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import image metadata (from Sangkloy et al. (2016))\n",
    "from photodraw32_metadata import metadata\n",
    "M = pd.DataFrame(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrieve images and features from lab3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill in name of directory that contains images\n",
    "prefix_dir = '../lab3'\n",
    "data_dir = os.path.join(prefix_dir,'images')\n",
    "feature_dir = os.path.join(prefix_dir,'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract pixel representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting pixel feature representation for ../lab3/images/n04587648_16394_window_31.png\n"
     ]
    }
   ],
   "source": [
    "## rescale to this imsize\n",
    "imsize = 64\n",
    "\n",
    "## get list of paths \n",
    "im_paths = [os.path.join(data_dir, path) for path in M.s3_filename.values]\n",
    "\n",
    "## init P pixel feature matrix\n",
    "PF = np.zeros((M.shape[0], imsize**2*3))\n",
    "num_pix_feats = PF.shape[1]\n",
    "\n",
    "## create new column that corresponds to image_id column in VGG metadata\n",
    "M['image_id'] = M.apply(lambda x: x['s3_filename'].split('.')[0], axis=1)\n",
    "\n",
    "## iterate over image paths and add pixel feature representation to P feature matrix\n",
    "for ind, path in enumerate(im_paths):\n",
    "    im = Image.open(path).resize((imsize, imsize), Image.ANTIALIAS)\n",
    "    vec = np.array(im).flatten()\n",
    "    PF[ind,:] = vec\n",
    "    print('Extracting pixel feature representation for {}'.format(path))\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "## join pixel feature matrix and metadata to form single PIXEL dataframe\n",
    "P = M.join(pd.DataFrame(PF))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract VGG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper function to perform channel normalization\n",
    "def normalize(X):\n",
    "    X = X - X.mean(0)\n",
    "    X = X / np.maximum(X.std(0), 1e-5)\n",
    "    return X\n",
    "\n",
    "## pre-extracted, only extract if you wish to overwrite\n",
    "extract=False\n",
    "if extract:\n",
    "    cmd_string = \"python extract_features.py --data={} --out_dir={}\".format(data_dir,feature_dir)\n",
    "    os.system(cmd_string)\n",
    "    \n",
    "## load in feature matrix and apply preprocessing (channel-wise normalization)\n",
    "VF = normalize(np.load(os.path.join(feature_dir,'FEATURES_FC6_IMAGES.npy')))\n",
    "num_vgg_feats = VF.shape[1]\n",
    "\n",
    "## load in metadata corresponding to VGG features\n",
    "VM = pd.read_csv(os.path.join(feature_dir,'METADATA_images.csv'))    \n",
    "\n",
    "## join feature matrix and metadata to form single VGG dataframe\n",
    "V = VM.join(pd.DataFrame(VF))\n",
    "\n",
    "## create new columns to make it easier to sort into categories alphabetically\n",
    "V['category'] = V.apply(lambda x: x['image_id'].split('_')[-2], axis=1)\n",
    "V.loc[V['category']=='(sedan)', 'category'] = 'car_(sedan)' ## deal with exception, so categories sort properly\n",
    "\n",
    "V['img_ind'] = VM.apply(lambda x: x['image_id'].split('_')[-1], axis=1)\n",
    "\n",
    "## sort rows by category, then by img_ind\n",
    "V.sort_values(by=['category','img_ind'], ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-nearest-neighbor (kNN) classification\n",
    "Recommended reading: https://cs231n.github.io/classification/ (markdown in this lab was adapted from these course notes)\n",
    "\n",
    "The _Nearest Neighbor_ classifier will take a *test image*, compare it to every single one of the *training images*, and predict the label of the closest training image.\n",
    "\n",
    "The _k-Nearest Neighbor_ classifier, instead of taking the single closest image in the training set, we will find the top k closest images, and have them vote on the label of the test image. In particular, when k = 1, we recover the Nearest Neighbor classifier. Intuitively, higher values of k have a smoothing effect that makes the classifier more resistant to outliers:\n",
    "\n",
    "![](https://cs231n.github.io/assets/knn.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the implementation of kNN classification from the [sci-kit learn](https://scikit-learn.org/stable/) library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## crossvalidation\n",
    "\n",
    "Generally speaking, we are looking for image classification methods that can generalize to new images. To measure how well kNN classification generalizes to new images, researchers will typically split their dataset into two sub-datasets: a **training dataset** and a **test dataset**. When reporting how well the classification method works, typically only performance on the \"held-out\" test dataset is given. To estimate how much uncertainty we have in our estimates of generalization, it is common to use not just one split of the data, but multiple splits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### writing a custom function to get splits from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(df, \n",
    "               train_prop=0.8, \n",
    "               random_seed=0,\n",
    "               replace=False,\n",
    "               group='category',\n",
    "               identifier='image_id'):\n",
    "\n",
    "    ## infer how many observations per group and use to \n",
    "    num_obs_per_group = int(df.groupby(group).size().mean())\n",
    "    size = int(train_prop * num_obs_per_group) ## how many obs do include in train split\n",
    "    replace = False  # with replacement\n",
    "\n",
    "    ## create splits\n",
    "    fn = lambda obj: obj.loc[np.random.RandomState(random_seed).choice(obj.index, size, replace),:]    \n",
    "    train_split = df.groupby(group, as_index=False).apply(fn)\n",
    "    common = df.merge(train_split,on=[identifier])\n",
    "    test_split = df[(~df.image_id.isin(common.image_id))]\n",
    "\n",
    "    ## sanity check, there is no overlap in image_id\n",
    "    assert len(np.intersect1d(train_split[identifier],test_split[identifier]))==0\n",
    "    \n",
    "    return train_split.reset_index(drop=True), test_split\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply get_splits function to get splits\n",
    "train, test = get_splits(V, train_prop=0.75, random_seed=0)\n",
    "\n",
    "## define training data for kNN classifier\n",
    "Xtrain = train[np.arange(num_vgg_feats)]\n",
    "ytrain = train['category'].values\n",
    "\n",
    "## define test data for kNN classifier\n",
    "Xtest = test[np.arange(num_vgg_feats)]\n",
    "ytest = test['category'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## create an instance of the kNN classifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71484375\n"
     ]
    }
   ],
   "source": [
    "## how well did we do at classifying images in the test set?\n",
    "print(clf.score(Xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold crossvalidation\n",
    "A common way to perform crossvalidation is known as **k-fold crossvalidation** (no relation to the k in k nearest neighbors). In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The k results can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on fold 1 = 0.695\n",
      "Accuracy on fold 2 = 0.711\n",
      "Accuracy on fold 3 = 0.703\n",
      "Accuracy on fold 4 = 0.773\n",
      "Mean accuracy = 0.721\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=4, random_state=1, shuffle=True)\n",
    "\n",
    "## entire dataset\n",
    "X = np.array(V[np.arange(num_vgg_feats)])\n",
    "y = V['category'].values\n",
    "\n",
    "## for each split get train/test indices\n",
    "counter = 0\n",
    "acc = []\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    Xtrain, Xtest = X[train_index], X[test_index]\n",
    "    ytrain, ytest = y[train_index], y[test_index]\n",
    "        \n",
    "    ## create an instance of the kNN classifier\n",
    "    clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    clf.fit(Xtrain, ytrain)\n",
    "\n",
    "    ## accuracy on test split\n",
    "    score = clf.score(Xtest, ytest)\n",
    "    acc.append(score)\n",
    "    \n",
    "    ## how well did we do at classifying images in the test set?\n",
    "    print('Accuracy on fold {} = {}'.format(counter+1, np.round(score,3)))\n",
    "    counter+=1\n",
    "print('Mean accuracy = {}'.format(np.round(np.mean(acc),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['airplane', 'airplane', 'airplane', ..., 'window', 'window',\n",
       "       'window'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix\n",
    "Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot confusion matrix\n",
    "from sklearn.metrics import plot_confusion_matrix        \n",
    "disp = plot_confusion_matrix(clf, Xtest, ytest,\n",
    "                             display_labels=np.unique(ytest),\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             normalize='true',\n",
    "                             include_values=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## support vector machine classification\n",
    "\n",
    "Although relatively straightforward to understand, kNN has a number of disadvantages:\n",
    "\n",
    "- The classifier must remember all of the training data and store it for future comparisons with the test data. This is space inefficient because datasets may easily be gigabytes in size.\n",
    "- Classifying a test image is expensive since it requires a comparison to all training images.\n",
    "\n",
    "Next, we'll explore a more powerful approach to image classification that is used much more widely. The approach will have two major components: a **score function** that maps the raw data to class scores, and a **loss function** that quantifies the agreement between the predicted scores and the ground truth labels. We will then cast this as an optimization problem in which we will minimize the loss function with respect to the parameters of the score function.\n",
    "\n",
    "The first component of this approach is to define the score function that maps the pixel values of an image to confidence scores for each class. We will develop the approach with a concrete example. As before, let’s assume a training dataset of images xi∈RD, each associated with a label yi. Here i=1…N and yi∈1…K. That is, we have N examples (each with a dimensionality D) and K distinct categories. \n",
    "\n",
    "**Linear classifier.** In this module we will start out with arguably the simplest possible function, a linear mapping:\n",
    "\n",
    "$$f(x_i,W,b)=Wx_i+b$$\n",
    "\n",
    "In the above equation, $x_i$ represents the image feature vector flattened out to a single column vector of shape [D x 1]. The matrix $W$ (of size $[K x D]$), and the vector $b$ (of size $[K x 1]$) are the parameters of the function. In `photodraw32`, $x_i$ either represents all pixels in the i-th image flattened into a single $[12288 x 1]$ column OR the 4096-dimensional feature vector extracted by VGG.\n",
    "\n",
    "$W$ is $[10 x D]$ and $b$ is $[32 x 1]$, so D numbers come into the function (the elements of the feature vector) and 32 numbers come out (the class scores). The parameters in W are often called the weights, and $b$ is called the bias vector because it influences the output scores, but without interacting with the actual data $x_i$. However, you will often hear people use the terms weights and parameters interchangeably.\n",
    "\n",
    "There are a few things to note:\n",
    "\n",
    "- First, note that the single matrix multiplication Wxi is effectively evaluating 10 separate classifiers in parallel (one for each class), where each classifier is a row of W.\n",
    "- Notice also that we think of the input data ($x_i$,$y_i$) as given and fixed, but we have control over the setting of the parameters W,b. Our goal will be to set these in such way that the computed scores match the ground truth labels across the whole training set. We will go into much more detail about how this is done, but intuitively we wish that the correct class has a score that is higher than the scores of incorrect classes.\n",
    "- An advantage of this approach is that the training data is used to learn the parameters W,b, but once the learning is complete we can discard the entire training set and only keep the learned parameters. That is because a new test image can be simply forwarded through the function and classified based on the computed scores.\n",
    "- Lastly, note that classifying the test image involves a single matrix multiplication and addition, which is significantly faster than comparing a test image to all training images.\n",
    "\n",
    "![](https://cs231n.github.io/assets/imagemap.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of mapping an image to class scores. For the sake of visualization, we assume the image only has 4 pixels (4 monochrome pixels, we are not considering color channels in this example for brevity), and that we have 3 classes (red (cat), green (dog), blue (ship) class). (Clarification: in particular, the colors here simply indicate 3 classes and are not related to the RGB channels.) We stretch the image pixels into a column and perform matrix multiplication to get the scores for each class. Note that this particular set of weights W is not good at all: the weights assign our cat image a very low cat score. In particular, this set of weights seems convinced that it's looking at a dog.\n",
    "\n",
    "**Analogy of images as high-dimensional points.** We can interpret each feature vector as a single point in a high-dimensional feature space (e.g. each image in photodraw32 is a point in 4096-dimensional space). Analogously, the entire dataset is a (labeled) set of points.\n",
    "\n",
    "Since we defined the score of each class as a weighted sum of all image pixels, each class score is a linear function over this space. We cannot visualize 4096-dimensional spaces, but if we imagine squashing all those dimensions into only two dimensions, then we can try to visualize what the classifier might be doing:\n",
    "\n",
    "![](https://cs231n.github.io/assets/pixelspace.jpeg)\n",
    "\n",
    "Cartoon representation of the image space, where each image is a single point, and three classifiers are visualized. Using the example of the car classifier (in red), the red line shows all points in the space that get a score of zero for the car class. The red arrow shows the direction of increase, so all points to the right of the red line have positive (and linearly increasing) scores, and all points to the left have a negative (and linearly decreasing) scores.\n",
    "\n",
    "As we saw above, every row of W is a classifier for one of the classes. The geometric interpretation of these numbers is that as we change one of the rows of W, the corresponding line in the pixel space will rotate in different directions. The biases b, on the other hand, allow our classifiers to translate the lines. In particular, note that without the bias terms, plugging in xi=0 would always give score of zero regardless of the weights, so all lines would be forced to cross the origin.\n",
    "\n",
    "**Interpretation of linear classifiers as template matching.** Another interpretation for the weights W is that each row of W corresponds to a template (or sometimes also called a prototype) for one of the classes. The score of each class for an image is then obtained by comparing each template with the image using an inner product (or dot product) one by one to find the one that “fits” best. With this terminology, the linear classifier is doing template matching, where the templates are learned. Another way to think of it is that we are still effectively doing Nearest Neighbor, but instead of having thousands of training images we are only using a single image per class (although we will learn it, and it does not necessarily have to be one of the images in the training set), and we use the (negative) inner product as the distance instead of the L1 or L2 distance.\n",
    "\n",
    "![](https://cs231n.github.io/assets/templates.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "In the previous section we defined a function from the pixel values to class scores, which was parameterized by a set of weights W. Moreover, we saw that we don’t have control over the data $(x_i,y_i)$ (it is fixed and given), but we do have control over these weights and we want to set them so that the predicted class scores are consistent with the ground truth labels in the training data.\n",
    "\n",
    "For example, going back to the example image of a cat and its scores for the classes “cat”, “dog” and “ship”, we saw that the particular set of weights in that example was not very good at all: We fed in the pixels that depict a cat but the cat score came out very low (-96.8) compared to the other classes (dog score 437.9 and ship score 61.95). We are going to measure our unhappiness with outcomes such as this one with a loss function (or sometimes also referred to as the cost function or the objective). Intuitively, the loss will be high if we’re doing a poor job of classifying the training data, and it will be low if we’re doing well.\n",
    "\n",
    "\n",
    "##### Multiclass Support Vector Machine loss\n",
    "There are several ways to define the details of the loss function. As a first example we will first develop a commonly used loss called the Multiclass Support Vector Machine (SVM) loss. The SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin Δ. Notice that it’s sometimes helpful to anthropomorphise the loss functions as we did above: The SVM “wants” a certain outcome in the sense that the outcome would yield a lower loss (which is good).\n",
    "\n",
    "Let’s now get more precise. Recall that for the i-th example we are given the pixels of image xi and the label yi that specifies the index of the correct class. The score function takes the pixels and computes the vector $f(x_i,W)$ of class scores, which we will abbreviate to s (short for scores). For example, the score for the j-th class is the j-th element: $s_j=f(x_i,W)_j$. The Multiclass SVM loss for the i-th example is then formalized as follows:\n",
    "\n",
    "$$L_i = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta) $$\n",
    "\n",
    "\n",
    "The Multiclass Support Vector Machine \"wants\" the score of the correct class to be higher than all other scores by at least a margin of delta. If any class has a score inside the red region (or higher), then there will be accumulated loss. Otherwise the loss will be zero. Our objective will be to find the weights that will simultaneously satisfy this constraint for all examples in the training data and give a total loss that is as low as possible.\n",
    "\n",
    "![](https://cs231n.github.io/assets/margin.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure accuracy of your SVM classifier applied to the VGG image features using `LinearSVC` (and applying stratified k-fold crossvalidation, as above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "## INSERT YOUR CODE HERE ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a confusion matrix for your SVM classifier applied to VGG image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now measure accuracy of your SVM classifier applied to the raw pixel representations of these images (applying the same crossvalidation as above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a confusion matrix for your SVM classifier applied to the raw pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT YOUR CODE HERE ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you notice about the differences between classification accuracy on VGG featueres vs. raw pixels? How do your observations relate to the visualizations you made in lab 3?\n",
    "\n",
    "_INSERT YOUR RESPONSE HERE_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression (\"softmax\") classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the SVM is one of two commonly seen classifiers. The other popular choice is the Logistic Regression classifier, which has a different loss function. If you’ve heard of the binary Logistic Regression classifier before, the Softmax classifier is its generalization to multiple classes. Unlike the SVM which treats the outputs $f(x_i,W)$ as (uncalibrated and possibly difficult to interpret) scores for each class, the Softmax classifier gives a slightly more intuitive output (normalized class probabilities) and also has a probabilistic interpretation that we will describe shortly. In the Softmax classifier, the function mapping $f(x_i;W)=Wx_i$ stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class and replace the hinge loss with a cross-entropy loss that has the form:\n",
    "\n",
    "$$L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) \\hspace{0.5in} \\text{or equivalently} \\hspace{0.5in} L_i = -f_{y_i} + \\log\\sum_j e^{f_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure accuracy of your logistic regression classifier applied to the VGG image features using LogisticRegression (and applying stratified k-fold crossvalidation, as above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "## INSERT YOUR CODE HERE ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a confusion matrix for your logistic regression classifier applied to VGG image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now measure accuracy of your logistic regression classifier applied to the raw pixel representations of these images (applying the same crossvalidation as above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a confusion matrix for your logistic regression classifier applied to raw pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you notice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
